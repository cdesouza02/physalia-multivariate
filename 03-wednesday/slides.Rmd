---
title: "Constrained ordination"
author: Gavin L. Simpson
date: "February 12, 2025"
fontsize: 10pt
classoption: "compress, aspectratio=169"
bibliography: "resources/vegan-refs.bib"
output:
  xaringan::moon_reader:
    css: ['default', 'https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css', 'slides.css']
    lib_dir: libs
    nature:
      titleSlideClass: ['inverse','middle','left',my-title-slide]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
      ratio: '16:9'
---
class: inverse middle center big-subsection

```{r setup-options, echo = FALSE, results = "hide", message = FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, dev = 'svg', echo = TRUE, message = FALSE, warning = FALSE,
                      fig.height=6, fig.width = 1.777777*6)
library("vegan")
library("ggplot2")
data(varespec)
data(varechem)

## plot defaults
theme_set(theme_minimal(base_size = 16, base_family = 'Fira Sans'))
```

# Welcome

---

# Today's topics

* Constrained ordination
    * Canonical Correspondence Analysis
	* Redundancy Analysis
	* Partial constrained ordination
* Model Building
    * Model selection
* Permutation tests

---
class: inverse middle center subsection

# Constrained Ordination

---
class: inverse middle center big-subsection

# CCA

---

# Canonical Correspondence Analysis

CCA is the constrained form of CA; fitted using `cca()`

Two interfaces for specifying models

* basic; `cca1 <- cca(X = varespec, Y = varechem)`
* formula; `cca1 <- cca(varespec ~ ., data = varechem)`

RDA is the constrained form of PCA; fitted using `rda()`

Formula interface is the more powerful &mdash; *recommended*

---

# Canonical Correspondence Analysis

```{r cca-model}
cca1 <- cca(varespec ~ ., data = varechem)
cca1
```

---

# Redundancy Analysis

```{r rda-model}
rda1 <- rda(varespec ~ ., data = varechem)
rda1
```

---

# The `cca.object`

 * Objects of class `"cca"` are complex with many components
 * Entire class described in `?cca.object`
 * Depending on what analysis performed some components may be `NULL`
 * Used for (C)CA, PCA, RDA, CAP (`capscale()`), and dbRDA (`dbrda()`)

---

# The `cca.object`

`cca1` has a large number of components

* **`$call`** how the function was called
* **`$grand.total`** in (C)CA sum of `rowsum`
* **`$rowsum`** the row sums
* **`$colsum`** the column sums
* **`$tot.chi`** total inertia, sum of Eigenvalues
* **`$pCCA`** Conditioned (partial-ed out) components
* **`$CCA`** Constrained components
* **`$CA`** Unconstrained components
* **`$method`** Ordination method used
* **`$inertia`** Description of what inertia is

---

# The `cca.object`

Depending on how one called `cca()` etc some of these components will be `NULL`

`$pCCA` is only filled in if a *partial* constrained ordination fitted

`rda()` returns objects with classes `"rda"` and `"cca"`, but in most cases those objects work like those of class `"cca"`

The Eigenvalues and axis scores are now spread about the `$CA` and `$CCA` components (also `$pCCA` if a *partial* CCA)

Thankfully we can use *extractor* functions to get at such things

---

# Eigenvalues

Use `eigenvals()` to extract Eigenvalues from a fitted ordination object

```{r eigenvals}
eigenvals(cca1)
```

---

# Example

- Fit a CCA model to the lichen pasture data. The model should include, N, P, and K only.
- Save the model in object `mycca1`
- How much variance is explained by this model?
- Extract the eigenvalues, how many constrained axes are there?

---

```{r your-turn-fit-cca-1}
library("vegan")
data(varechem, varespec)
```

--

```{r your-turn-fit-cca-2}
mycca1 <- cca(varespec ~ N + P + K, data = varechem)
mycca1
```

---

```{r your-turn-fit-cca-3}
ev <- eigenvals(mycca1, model = "constrained")
head(ev)
length(ev)
```


---

# Extracting axis scores

To extract a range of scores from  a fitted ordination use `scores()`

* takes an ordination object as the first argument
* `choices` &mdash; which axes? Defaults to `c(1,2)`
* `display` &mdash; which type(s) of scores to return
    - `"sites"` or `"wa"`: scores for samples in response matrix
    - `"species"`: scores for variables/columns in response
    - `"lc"`: linear combination site scores
    - `"bp"`: biplot scores (coords of arrow tip)
    - `"cn"`: centroid scores (coords of factor centroids)

---

# Extracting axis scores

```{r scores}
str(scores(cca1, choices = 1:4, display = c("species","sites")), max = 1)
head(scores(cca1, choices = 1:2, display = "sites"))
```

---

# Scalings&hellip;

When we draw the results of many ordinations we display 2 or more sets of data

Can't display all of these and maintain relationships between the scores

*Solution* scale one set of scores relative to the other via the `scaling` argument

---

# Scalings&hellip;

* `scaling = 1` &mdash; Focus on sites, scale site scores by $\lambda_i$
* `scaling = 2` &mdash; Focus on species, scale species scores by $\lambda_i$
* `scaling = 3` &mdash; Symmetric scaling, scale both scores by $\sqrt{\lambda_i}$
* `scaling = -1` &mdash; As above, but
* `scaling = -2` &mdash; For `cca()` multiply results by $\sqrt{(1/(1-\lambda_i))}$
* `scaling = -3` &mdash; this is Hill's scaling
* `scaling < 0` &mdash; For `rda()` divide species scores by species' $\sigma$
* `scaling = 0` &mdash; raw scores

```{r scaling-example, results = "hide"}
scores(cca1, choices = 1:2, display = "species", scaling = 3)
```

---

# Scalings&hellip;

Thankfully we can use alternative descrpitors to extract scores:

- `"none"`
- `"sites"`
- `"species"`
- `"symmetric"`

Two modifiers select negative scores depending on whether the model is CCA or RDA:

- `hill = TRUE`
- `correlation = TRUE`

---

# Example

- Using the CCA model you fitted, extract the site scores for axes 2 and 3 with Hill's scaling, focusing on the sites

--

```{r your-turn-cca-4}
scrs <- scores(mycca1, display = "sites", choices = c(2,3),
               scaling = "sites", hill = TRUE)
head(scrs)
```

---

# Partial constrained ordinations

*Partial* constrained ordinations remove the effect of one or more variables *then* fit model of interest

Argument `Z` is used for a data frame of variables to partial out

```{r partial-ordination-1}
pcca <- cca(X = varespec,
            Y = varechem[, "Ca", drop = FALSE],
            Z = varechem[, "pH", drop = FALSE])
```
Or with the formula interface use the `Condition()` function

```{r partial-ordination-2}
pcca <- cca(varespec ~ Ca + Condition(pH), data = varechem) ## easier!
```

---

# Partial constrained ordinations

```{r partial-ordination-3}
pcca <- cca(varespec ~ Ca + Condition(pH), data = varechem) ## easier!
pcca
```

---

# Triplots

.row[
.col-6[

Triplots will generally produce a mess; we can really only display a couple of bits approximately anyway Trying to cram three things in is a recipe for a mess&hellip; but we can do it
]
.col-6[
```{r triplot-1, fig.height = 6, fig.width = 6}
plot(cca1)
```
]
]

---

# LC vs WA scores

In constrained ordinations there are two sets of "site" scores

1. *linear combination* scores
2. *weighted average* (or weighted summation) scores

The LC scores are weighted & scaled versions of the constraints (predictors)

The WA scores are weighted averages of the species (responses) that are as close to the LC scores as possible

---

# LC vs WA scores

.row[

Because the LC scores are based on the constraints you can get odd results if you plot them for some models

.col-6[

```{r lc-vs-wa-scores-1, echo = TRUE, fig.keep = "none"}
# example from Design Decision vignette
data(dune, dune.env, package = "vegan")
ord <- cca(dune ~ Moisture, data = dune.env)
plot(ord, display = "lc", type = "points")
```

]

.col-6[
```{r lc-vs-wa-scores-1, echo = FALSE}
```
]
]

`Moisture` doesn't vary *within* observations of the same `Moisture` class (or level) so no variation in the LC scores for samples in each class.


---

# LC vs WA scores

.row[

Weighted average scores include the variation in the species composition

.col-6[

```{r lc-vs-wa-scores-2, echo = TRUE, fig.keep = "none"}
# example from Design Decision vignette
data(dune, dune.env, package = "vegan")
ord <- cca(dune ~ Moisture, data = dune.env)
plot(ord, display = "wa", type = "points")
ordispider(ord, col = "red")
text(ord, display = "cn", col = "blue")
```
]

.col-6[
```{r lc-vs-wa-scores-2, echo = FALSE}
```
]
]

---
class: inverse middle center subsection

# Model building

---

# Building constrained ordination models

If we don't want to think it's easy to fit a poor model with many constraints &mdash; That's what I did with `cca1` and `rda1`

Remember, CCA and RDA are *just regression methods* &mdash; everything you know about regression applies here

A better approach is to *think* about the important variables and include only those

The formula interface allows you to create interaction or quadratic terms easily (though be careful with latter)

It also handles factor or class constraints automatically unlike the basic interface

---

# Building constrained ordination models

```{r cca-model-build1}
vare.cca <- cca(varespec ~ Al + P*(K + Baresoil), data = varechem)
vare.cca
```

---

# Building constrained ordination models

For CCA, RDA etc we have little choice but to do

 1. Fit well-chosen set of candidate models & compare, or
 2. Fit a *full* model of well-chosen variables & then do stepwise selection

Automatic approaches to model building should be used cautiously!

The standard `step()` function can be used as *vegan* provides two helper methods, `deviance()` and `extractAIC()`, used by `step()`

*vegan* also provides methods for class `"cca"` for `add1()` and `drop1()`

---

# Variance inflation factors

*Linear* dependencies between constraints can be investigated with VIF

VIF is a measure of how much the variance of $\hat{\beta}_j$ is inflated by presence of other covariates

Lots of rules of thumb

 * VIF >= 20 indicates *strong collinearity* in constraints
 * VIF >= 10 potentially of concern & should be looked at

Computed via `vif.cca()`

```{r vif-cca1}
vif.cca(cca1)
```

---

# Stepwise selection in CCA

`step()` uses AIC which is a fudge for RDA/CCA &mdash; use `ordistep()`

1. Define an upper and lower model scope, say the full model and the null model
2. To step from the lower scope or null model we use

```{r stepwise-1}
upr <- cca(varespec ~ ., data = varechem)
lwr <- cca(varespec ~ 1, data = varechem)
set.seed(1)
mods <- ordistep(lwr, scope = formula(upr), trace = 0)
```

`trace = 0` is used here to turn off printing of progress

Permutation tests are used (more on these later); the theory for an AIC for ordination is somewhat loose

---

# Stepwise selection in CCA

The object returned by `step()` is a standard `"cca"` object with an extra component `$anova`

```{r stepwise-cca}
mods
```

---

# Stepwise selection in CCA

The `$anova` component contains a summary of the steps involved in automatic model building

```{r stepwise-anova}
mods$anova
```

1. `Al` added first, then
2. `P`, followed by
3. `K`, then stopped

---

# Stepwise selection in CCA

Step-wise model selection is fairly fragile; if we start from the full model we won't end up with the same final model

```{r stepwise-reverse, results = "hide"}
mods2 <- step(upr, scope = list(lower = formula(lwr), upper = formula(upr)), trace = 0,
              test = "perm")
mods2
```

---

# Stepwise selection in CCA

```{r stepwise-reverse}
```

---

# Adjusted $R^2$ for ordination models

Ordinary $R^2$ is biased for the same reasons as for a linear regression

 * adding a variable to constraints will increase $R^2$
 * the larger the number of constraints in the model the larger $R^2$, is due to random correlations

Can attempt to account for this bias via an *adjusted* $R^2$ measure

---

# Adjusted $R^2$ for ordination models

Can attempt to account for this bias via an *adjusted* $R^2$ measure

$$R^2_{adj} = 1 - \frac{n - 1}{n - m - 1}(1 - R^2)$$

* $n$ is number of samples $m$ is number of constraints (model degrees of freedom)
* Can be used up to $\sim M > n/2$ before becomes too conservative
* Can be negative

```{r rsq-cca1}
RsquareAdj(cca1)
```

---

# Stepwise selection via adjusted $R^2$

Problems with stepwise selection are myriad. Affects RDA, CCA, etc

Blanchet *et al* (2008) proposed a two-step solution for models where $R^2_{adj}$ makes sense

---

# Stepwise selection via adjusted $R^2$

* *Global test* of all constraints
    * Proceed **only** if this test is significant
    * Helps prevent inflation of overall type I error
* Proceed with forward selection, but with *two* stopping rules
    * Usual significance threshold $\alpha$
    * The global $R^2_{adj}$
    * Stop if next candidate model is non-significant or if $R^2_{adj}$ exceeds the global $R^2_{adj}$

Available in `ordiR2step()`

---

# Stepwise selection via adjusted $R^2$

```{r stopping-rules}
ordiR2step(lwr, upr, trace = FALSE)
```

---
class: inverse middle center subsection

# Permutation tests

---

# Permutation tests in vegan

RDA has lots of theory behind it, CCA bit less. However, ecological/environmental data invariably violate what little theory we have

Instead we use permutation tests to assess the *importance* of fitted models &mdash; the data are shuffled in some way and the model refitted to derive a Null distribution under some hypothesis of *no effect*

---

# Permutation tests in vegan

What *is* shuffled and *how* is of **paramount** importance for the test to be valid

* No conditioning (partial) variables then rows of the species data are permuted
* With conditioning variables, two options are available, both of which *permute residuals* from model fits
    * The *full model* uses residuals from model $Y = X + Z + \varepsilon$
    * The *reduced model* uses residuals from model $Y = Z + \varepsilon$
* In **vegan** which is used can be set via argument `model` with `"direct"`, `"full"`, and `"reduced"` respectively

---

# Permutation tests in vegan

A test statistic is required, computed for observed model & each permuted model

**vegan** uses a pseudo $F$ statistic

$$F=\frac{\chi^2_{model} / df_{model}}{\chi^2_{resid} / df_{resid}}$$

Evaluate whether $F$ is unusually large relative to the null (permutation) distribution of $F$

---

# Permutation tests in vegan

.row[
.col-6[
```{r permustats-1, results = "hide"}
pstat <- permustats(anova(cca1))
summary(pstat)
```
.small[
```{r permustats-1, echo = FALSE}
```
]
]
.col-6[
```{r permustats-2, fig.width = 6, fig.height = 6}
densityplot(pstat)
```
]
]

???

The summary method of permustats estimates the standardized effect sizes (SES) as the difference
of observed statistic and mean of permutations divided by the standard deviation of permutations
(also known as z-values). It also prints the the mean, median, and limits which contain interval
percent of permuted values. With the default (interval = 0.95), for two-sided test these are (2.5%,
97.5%) and for one-sided tests either 5% or 95% quantile and the p-value depending on the test
direction. The mean, quantiles and z values are evaluated from permuted values without observed
statistic, but the p-value is evaluated with the observed statistic. 

---

# Permutation tests in vegan: `anova()`

* The main user function is the `anova()` method
* It is an interface to the lower-level function `permutest.cca()`
* At its most simplest, the `anova()` method tests whether the **model** as a whole is significant

---

# Permutation tests in vegan: `anova()`

$$F = \frac{1.4415 / 14}{0.6417 / 9} = 1.4441$$

```{r cca-anova}
set.seed(42)
(perm <- anova(cca1))
```

---

# Permutation tests in vegan: `anova()`

`anova.cca()` has a number of arguments

```{r anova-args}
args(anova.cca)
```

`object` is the fitted ordination

`permutations` controls what is permuted and how

`by` determines what is tested; the default is to test the model

---

# Types of permutation test in vegan

A number of types of test can be envisaged

 * Testing the overall significance of the model
 * Testing constrained (canonical) axes
 * Testing individual model terms *sequentially*
 * The *marginal* effect of a single variable

The first is the default in `anova()`

The other three can be selected via the argument `by`

---

# Testing canonical axes

* The constrained (canonical) axes can be individually tests by specifying `by = "axis"`
* The first axis is tested in terms of variance explained compared to residual variance
* The second axis is tested after partialling out the first axis&hellip;
* and so on

---

# Testing canonical axes

```{r anova-by-axis}
set.seed(1)
anova(mods, by = "axis")
```

---

# Testing terms sequentially

* The individual terms in the model can be tested using `by = "terms"`
* The terms are assessed in the order they were specified in the model, sequentially from first to last
* Test is of the additional variance explained by adding the $k$th variable to the model
* **Ordering of the terms** will affect the results

---

# Testing terms sequentially

```{r anova-by-term}
set.seed(5)
anova(mods, by = "terms")
```

---

# Testing terms marginal effects

* The marginal *effect* of a model term can be assessed using `by = "margin"`
* The marginal *effect* is the effect of a particular term when all other model terms are included in the model

---

# Testing terms marginal effects

```{r anova-by-margin}
set.seed(10)
anova(mods, by = "margin")
```

---

# Spring meadow vegetation

Example & data taken from Leps & Smilauer (2014), Case Study 2

Spring fen meadow vegetation in westernmost Carpathian mountains

```{r meadows-setup}
# load vegan, dplyr & readr
library("vegan"); library("dplyr"); library("readr")

# load the data
spp <- read_csv("https://bit.ly/meadows-species") %>%
    rename("sample_id" = "...1") %>%
    tibble::column_to_rownames("sample_id")
env <- read_csv("https://bit.ly/meadows-env") %>%
    rename("sample_id" = "...1")
```

---

# Spring meadow vegetation

CCA a reasonable starting point as the gradient is long here (check with `decorana()` if you want)

```{r meadows-cca-full}
m1 <- cca(spp ~ ., data = env)
set.seed(32)
anova(m1)
```

---

# Spring meadow vegetation

.row[
.col-6[
```{r meadows-cca-full-triplot, fig.show = "hide"}
plot(m1)
```
]
.col-6[

```{r meadows-cca-full-triplot, fig.height = 6, fig.width = 6, echo = FALSE}
```
]
]

---

# Spring meadow vegetation

```{r meadows-cca-stepwise}
set.seed(67)
lwr <- cca(spp ~ 1, data = env)
( m2 <- ordistep(lwr, scope = formula(m1), trace = FALSE) )
```

---

# Spring meadow vegetation

.row[
.col-6[
```{r meadows-cca-reduced-triplot, fig.show = "hide"}
plot(m2)
```
]
.col-6[

```{r meadows-cca-reduced-triplot, fig.height = 6, fig.width = 6, echo = FALSE}
```
]
]

---

# Spring meadow vegetation

```{r meadows-cca-anova}
m2$anova
```

---

# Spring meadow vegetation

Alternative is RDA with a transformation

.row[
.col-6[
```{r meadows-rda}
spph <- decostand(spp, method = "hellinger")
m3 <- rda(spph ~ ., data = env)
lwr <- rda(spph ~ 1, data = env)
m4 <- ordistep(lwr, scope = formula(m3),
               trace = FALSE)
```
]
.col-6[
.small[
```{r meadows-rda-print}
m4
```
]
]
]
---

# Spring meadow vegetation

.row[
.col-6[
```{r meadows-rda-reduced-triplot, fig.show = "hide"}
plot(m4)
```
]
.col-6[

```{r meadows-rda-reduced-triplot, fig.height = 6, fig.width = 6, echo = FALSE}
```
]
]

---

# Spring meadow vegetation

Stepwise using $R^2_{adj}$

```{r meadows-rda-adjrsquare}
m5 <- ordiR2step(lwr, scope = formula(m3), trace = FALSE)
m5$anova
```

---
class: inverse middle center big-subsection

# PERMANOVA

---

# MANOVA

MANOVA is the multivariate form of ANOVA

* Multivariate response data
* Categorical predictor variables

Decompose variation in the responses into

1. variation within groups
2. variation between groups

Test to see if two is unusually large relative to H<sub>0</sub>

---

# PERMANOVA

Doing that test requires lots of assumptions that rarely hold for ecological data

PERMANOVA: Permutational multivariate analysis of variance

Avoids most of these issues through the use of permutation tests

Directly decomposes a dissimilarity matrix into

1. variation within groups
2. variation between groups

---

# PERMANOVA *sensu stricto*

*vegan* has four different ways to do essentially do this kind of analysis

1. `adonis()` &mdash; implements Anderson (2001)
2. `adonis2()` &mdash; implements McArdle & Anderson (2001)
3. `dbrda()` &mdash; implementation based on McArdle & Anderson (2001)
4. `capscale()` &mdash; implements Legendre & Anderson (1999)

Be careful with `adonis()` as it allows only sequential tests

A difference between the functions is how they treat negative eigenvalues

---

# The PERMANOVA idea

.center[
```{r permanova-idea-plot, echo = FALSE}
data(varespec)

## Bray-Curtis distances between samples
dis <- vegdist(varespec)

## First 16 sites grazed, remaining 8 sites ungrazed
groups <- factor(c(rep(1,16), rep(2,8)), labels = c("grazed","ungrazed"))

## Calculate multivariate dispersions
mod <- betadisper(dis, groups)
plot(mod)
```
]

---

# PERMANOA &mdash; `adonis2()`

```{r adonis2-by-terms}
data(dune, dune.env)
adonis2(dune ~ Management * A1, data = dune.env, by = "terms")
```

---

# PERMANOA &mdash; `adonis2()`

```{r adonis2-by-terms-flipped}
data(dune, dune.env)
adonis2(dune ~ A1 * Management, data = dune.env, by = "terms")
```

---

# PERMANOA &mdash; `adonis2()`

```{r adonis2-by-margin}
data(dune, dune.env)
adonis2(dune ~ Management * A1, data = dune.env, by = "margin")
```

--

The interaction is the only term that isn't *marginal* to other terms; not significant

---

# PERMANOA &mdash; `adonis2()`

```{r adonis2-margin-2}
adonis2(dune ~ Management + A1, data = dune.env, by = "margin")
```

---

# The dispersion problem

Anderson (2001) noted that PERMANOVA could confound *location* & *dispersion* effects

If one or more groups are more variable &mdash; dispersed around the centroid &mdash; than the others, this can result in a false detection of a difference of means &mdash; a *location* effect

Same problem affects *t* tests, ANOVA

Warton et al (2012)
Anderson & Walsh (2013)
Anderson *et al* (2017)

---

# Dispersion

.center[
```{r permanova-idea-plot, echo = FALSE}
```
]

---

# Test for dispersion effects

Marti Anderson (2006) developed a test for multivariate dispersions &mdash; PERMDISP2

1. Calculate how far each observation is from its group median (or centroid)
2. Take the absolute values of these distances-to-medians
3. Do an ANOVA on the absolute distances with the *groups* as covariates
4. Test the H<sub>0</sub> of equal absolute distances to median among groups using a permutation test

In *vegan* this is `betadisper()`

---

# Test for dispersion effects

.row[
.col-6[
.not-so-small[
```{r permdisp}
data(varespec)
dis <- vegdist(varespec) # Bray-Curtis distances
## First 16 sites grazed, remaining 8 sites ungrazed
groups <- factor(c(rep(1,16), rep(2,8)),
                 labels = c("grazed","ungrazed"))

mod <- betadisper(dis, groups)
mod
```
]
]
.col-6[
```{r permdisp-plot, fig.height = 6, fig.width = 6}
boxplot(mod)
```
]
]

---

# Test for dispersions

.row[

.col-6[
.smaller[
```{r permdisp-anova}
set.seed(25)
permutest(mod)
```
]
]

.col-6[
```{r permdisp-plot-it, fig.width = 6, fig.height = 6}
plot(mod)
```

]
]

---

# Test for dispersions

```{r permdisp-anova-2}
set.seed(4)
permutest(mod, pairwise = TRUE)
```

---

# Test for locations with non-equal dispersion?

Marti Anderson & colleagues (2017) have proposed a solution that is related to the Berens-Fisher problem

This is in Primer but not yet in *vegan*

<https://github.com/vegandevs/vegan/issues/344>

---
class: inverse middle center big-subsection

# Distance-based RDA

---

# Distance-based RDA

Multiple models that all do something similar

1. `adonis()` (deprecated)
2. `adonis2()`
3. `capscale()`
4. `dbrda()`

They all do essentially the same thing, but they do it differently & have slightly different behaviour

---

# Distance-based RDA

Distance-based RDA (db-RDA) is a constrained form of principal coordinates analysis (PCO)

It is similar to RDA but allows for non-Euclidean dissimilarity indices

In *vegan*, db-RDA is implemented in `dbrda()`

---

# Constrained analysis of principal coordinates

`capscale()` is *another* constrained form of PCO due to Legendre & Anderson (1999)

It is *very* similar to `dbrda()`

---

# Constrained analysis of principal coordinates

`capscale()` works by

1. convert the response data into dissimilarities
2. apply PCO on the dissimilarities, take the PCO sample (site) scores as *new* response data
3. fit `rda()` to the *new* response data and predictor variables as constraints

Essentially, we embed the dissimilarities in a Euclidean space using PCO, and then we use RDA on this highly transformed response data

---

# Distance-based RDA

db-RDA foregoes step 2., and directly decomposes the dissimilarities into components explained by each term in the model

Negative eigenvalues resulting from non-metric dissimilarity coefficients are handled via

1. square-root transform of the dissimilarities, or
2. adding a constant to the dissimilarities using methods `"lingoes"` (default, preferred) or `"cailliez"`

db-RDA is based on the ideas in McArdle & Anderson (2001)

--

Err&hellip; isn't that what `adonis2()` was developed to do?

--

*Yes*, but&hellip;

---

# Distance-based RDA

`adonis2()` was a ground up redevelopment of the `adonis()` implementation and as such it retains many of the arguments and concepts of PERMANOVA, just updated to use the direct decomposition of dissimilarities

`dbrda()` inherits from `rda()` and `cca()` and as a result has expanded set of capability

`dbrda()` can use `Condition()` in the formula to fit partial db-RDA

`Condition()` is often needed to provide correct restricted permutation tests

---

# Distance-based RDA

The equivalent model to `adonis2()` in `dbrda()`-form is

```{r adonis2-by-margin-as-db-rda}
data(dune, dune.env)
dune_dbrda <- dbrda(dune ~ Management * A1, data = dune.env,
    distance = "bray")
```

because they have different default dissimilarities

---

# References

.small[
* Anderson, M.J., 2001. A new method for non-parametric multivariate analysis of variance. Austral Ecol. 26, 32&ndash;46
* Anderson, M.J., 2006. Distance-based tests for homogeneity of multivariate dispersions. Biometrics 62, 245&ndash;253
* Anderson, M.J., Walsh, D.C.I., 2013. PERMANOVA, ANOSIM, and the Mantel test in the face of heterogeneous dispersions: What null hypothesis are you testing? Ecol. Monogr. 83, 557&ndash;574
* Anderson, M.J., Walsh, D.C.I., Robert Clarke, K., Gorley, R.N., Guerra-Castro, E., 2017. Some solutions to the multivariate Behrens-Fisher problem for dissimilarity-based analyses. Aust. N. Z. J. Stat. 59, 57&ndash;79
* Blanchet, F.G., Legendre, P., Borcard, D., 2008. Forward selection of explanatory variables. Ecology 89, 2623&ndash;2632
* Legendre, P., Anderson, M.J., 1999. Distance-based redundancy analysis: testing multispecies responses in multifactorial ecological experiments. Ecol. Monogr. 69, 1&ndash;24
* McArdle, B.H., Anderson, M.J., 2001. Fitting Multivariate Models to Community Data: A Comment on Distance-Based Redundancy Analysis. Ecology 82, 290&ndash;297
* Warton, D.I., Wright, S.T., Wang, Y., 2012. Distance-based multivariate analyses confound location and dispersion effects. Methods Ecol. Evol. 3, 89&ndash;101
]
